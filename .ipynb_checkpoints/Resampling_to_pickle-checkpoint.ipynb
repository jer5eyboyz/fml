{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import glob\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import seaborn as sns\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "path ='full_features'\n",
    "allFiles = glob.glob(path + \"/2017.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, encoding='iso-8859-1')\n",
    "    list_.append(df)\n",
    "df = pd.concat(list_)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def reduce_genres(gen):\n",
    "    genre = re.sub(\"[^a-zA-Z0-9]\",\" \",gen).lower().split()\n",
    "    genre = [i for i in genre if i not in set(stopwords.words(\"english\"))]\n",
    "    mode1 = str(stats.mode(genre)).split('[')[1].split(']')[0]\n",
    "    return mode1\n",
    "def bagwords (df, maxFeatureNum):\n",
    "    \n",
    "    listname = ['artist_genres', 'album_name', 'song_name', 'year', 'month']\n",
    "#     listname = ['artist_genres', 'album_name', 'song_name', 'year']\n",
    "\n",
    "    dicname = {name: [] for name in listname}\n",
    "    dicname['song_id'] = []\n",
    "    \n",
    "    listid = []\n",
    "    \n",
    "    dicdf = {}\n",
    "    vol = {}\n",
    "    print(df.shape)\n",
    " \n",
    "        \n",
    "    for name in listname:\n",
    "        \n",
    "        for idx in range(df[name].size):\n",
    "            \n",
    "            l = df[name][idx]\n",
    "          \n",
    "            r = names_to_words(l)\n",
    "            \n",
    "            dicname[name].append(r)     \n",
    "            \n",
    "        \n",
    "#         vectorizer = CountVectorizer(analyzer='word',max_features=30)\n",
    "#         vectorizer = CountVectorizer(analyzer='word',max_features=None)\n",
    "        vectorizer = CountVectorizer(analyzer='word',max_features=maxFeatureNum)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        feature = vectorizer.fit_transform(dicname[name]).toarray().tolist()\n",
    "        vol[name] = vectorizer.get_feature_names()\n",
    "        \n",
    "        \n",
    "        dicdf[name] = pd.DataFrame({name: feature}) \n",
    "        \n",
    "        dicdf[name] = dicdf[name][name].apply(pd.Series)\n",
    "        ### dicdf[name] already lose column tag name, but change into 0 1 2 \n",
    "        \n",
    "        l = len(dicdf[name].columns)\n",
    "        \n",
    "        dicdf[name].columns = [(name + '_' +vol[name][x]) for x in range(l)]\n",
    "        \n",
    "        print('good', name)\n",
    "     \n",
    "   \n",
    "    \n",
    "    for idx in range(df['song_id'].size):\n",
    "        sid = df['song_id'][idx]\n",
    "        listid.append(sid)\n",
    "       \n",
    "            \n",
    "        dicdf['song_id'] = pd.DataFrame({'song_id':listid}) \n",
    "    result = pd.concat(dicdf.values(), axis =1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def names_to_words(names):\n",
    "    words = re.sub(\"[^a-zA-Z0-9]\",\" \",names).lower().split()\n",
    "    \n",
    "    words = [i for i in words if i not in set(stopwords.words(\"english\"))]\n",
    "    ## Need join as string for countvectorizer!\n",
    "    return (\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop incomplete date success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:250: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if len(row['album_release_date'].split('-')) < 2:\n",
    "        print(row['album_release_date'])\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "print('drop incomplete date success')\n",
    "        \n",
    "df['year'] = [x.split('-')[0] for x in df['album_release_date']]\n",
    "df['month'] = [x.split('-')[1] for x in df['album_release_date']]\n",
    "\n",
    "df = df.drop(['album_genres', 'artist_name'],axis =1)\n",
    "\n",
    "df['explicit'] = df['explicit'].map( {True: 1, False: 0} ).astype(int) \n",
    "\n",
    "z = df['popularity'].quantile(0.8)\n",
    "df['class'] = df['popularity'].apply(lambda x: 1 if x >= z else 0)\n",
    "\n",
    "df = df[(df.astype(str)['artist_genres'] != '[]')].reset_index()\n",
    "\n",
    "df['reduced_genres'] = df['artist_genres'].apply(lambda x: reduce_genres(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######### get seperate genres ########\n",
    "# genredf = df['reduced_genres']\n",
    "\n",
    "# genredf.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5926, 36)\n",
      "good artist_genres\n",
      "good album_name\n",
      "good song_name\n",
      "good year\n",
      "good month\n"
     ]
    }
   ],
   "source": [
    "df1 = bagwords(df, 30)\n",
    "# df1 = bagwords(df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5926, 349)\n"
     ]
    }
   ],
   "source": [
    "df = df.merge(df1, on='song_id', how='outer')\n",
    "\n",
    "print(df.shape)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sample 5926\n",
      "up sample df 9290\n",
      "down sample df 2562\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "print('original sample', len(df['class']))\n",
    "################## upsample minority ####################\n",
    "\n",
    "df_majority = df[df['class']==0]\n",
    "df_minority = df[df['class']==1]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123)\n",
    "#random_state : int, RandomState instance or None, optional (default=None)\n",
    "#The seed of the pseudo random number generator to use when shuffling the data. \n",
    "#If int, random_state is the seed used by the random number generator; \n",
    "#If RandomState instance, random_state is the random number generator; \n",
    "#If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "# Display new class counts\n",
    "df_upsampled['class'].value_counts()\n",
    "print('up sample df', len(df_upsampled['class']))\n",
    "\n",
    "################# downsample majority ###############\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=len(df_minority),     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled['class'].value_counts()\n",
    "print('down sample df', len(df_downsampled['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sample ratio\n",
      "0    4645\n",
      "1    1281\n",
      "Name: class, dtype: int64\n",
      "\n",
      "down sample ratio\n",
      "1    1281\n",
      "0    1281\n",
      "Name: class, dtype: int64\n",
      "\n",
      "up sample ratio\n",
      "1    4645\n",
      "0    4645\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('original sample ratio')\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "print('\\ndown sample ratio')\n",
    "print(df_downsampled['class'].value_counts())\n",
    "\n",
    "print('\\nup sample ratio')\n",
    "print(df_upsampled['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf_downsampled = df_downsampled.drop(['Unnamed: 0', 'song_id', 'artist_id','album_id','song_name',\n",
    "            'album_name','uri', 'type', 'track_href',\n",
    "            'analysis_url','artist_genres','album_release_date','popularity', 'artist_popularity', \n",
    "            'album_popularity','index','reduced_genres'],axis=1) \n",
    "traindf_upsampled = df_upsampled.drop(['Unnamed: 0', 'song_id', 'artist_id','album_id','song_name',\n",
    "            'album_name','uri', 'type', 'track_href',\n",
    "            'analysis_url','artist_genres','album_release_date','popularity', 'artist_popularity', \n",
    "            'album_popularity','index','reduced_genres'],axis=1) \n",
    "traindf = df.drop(['Unnamed: 0', 'song_id', 'artist_id','album_id','song_name',\n",
    "            'album_name','uri', 'type', 'track_href',\n",
    "            'analysis_url','artist_genres','album_release_date','popularity', 'artist_popularity', \n",
    "            'album_popularity','index','reduced_genres'],axis=1) \n",
    "\n",
    "pickleDf_downsampled = traindf_downsampled\n",
    "pickleDf_upsampled = traindf_upsampled\n",
    "pickleDf = traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# #### dump different sampling df ###\n",
    "pickleDf.to_pickle('pickles/2017_original_sample_max_feature.p')\n",
    "pickleDf_downsampled.to_pickle('pickles/2017_down_sample_max_feature.p')\n",
    "pickleDf_upsampled.to_pickle('pickles/2017_up_sample_max_feature.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
